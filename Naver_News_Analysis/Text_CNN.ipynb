{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Analysis_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesahwi/TextAnlaysis/blob/master/Naver_News_Analysis/Text_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSyxV5bvuWky",
        "colab_type": "text"
      },
      "source": [
        "Text CNN implementation on analyzing Korean news articles <br>\n",
        "(In this particular example, we will tell '조선일보' and '한겨례' political articles apart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vc62EPBuXB0",
        "colab_type": "text"
      },
      "source": [
        "Install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n72_dHmVy4h",
        "colab_type": "code",
        "outputId": "d60883d7-2b77-4cdd-f721-90d0897e62c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!apt-get install python3-dev; pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6IJPq6uo4S",
        "colab_type": "text"
      },
      "source": [
        "Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJ2FcVPMkBT",
        "colab_type": "code",
        "outputId": "796d8bef-e99a-409a-8555-ee9467838b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkwSeburusgA",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zzufRUV5ZW",
        "colab_type": "code",
        "outputId": "4d0e7ca8-8f6f-4896-873a-048e3775ebd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import sklearn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import glob\n",
        "import warnings, os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from konlpy.tag import *\n",
        "\n",
        "# 아래와 같은 분석기가 있음.\n",
        "hnm = Hannanum()\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "\n",
        "base_dir = 'gdrive/Shared drives/텍스트마이닝/News Analysis/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-8kPzT4uwhI",
        "colab_type": "text"
      },
      "source": [
        "Import Korean stop words, found in 'https://www.ranks.nl/stopwords/korean'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP6Th-b8WDR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(base_dir+'stopwords.txt', 'r')\n",
        "stopwords = f.read()\n",
        "stopwords = stopwords.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiuTtjKZvTcL",
        "colab_type": "text"
      },
      "source": [
        "Define functions and objects that will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx5MheoPWYoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess, single documents, not whole corpus.\n",
        "# This function uses nouns. For a more sophisticated analysis, using morphs is recommended\n",
        "def preprocess_single_doc(text, tokenizer_type = 1):\n",
        "  \n",
        "  #choose tokenizer\n",
        "  if tokenizer_type == 1:\n",
        "    tokenizer = Hannanum()\n",
        "  elif tokenizer_type == 2:\n",
        "    tokenizer = Kkma()\n",
        "  elif tokenizer_type == 3:\n",
        "    tokenizer = Okt()\n",
        "    \n",
        "  #tokenize\n",
        "  tokens = tokenizer.nouns(text)\n",
        "  \n",
        "  #remove short words, but probably should not apply to Korean\n",
        "  #tokens = [token for token in tokens if len(token) > 1] \n",
        "  \n",
        "  #stop words\n",
        "  my_stopwords = ['조선일보', '조선닷컴', '닷컴', 'Chosun', 'Copyrights', '&', '바로가기', '기자', '구독', '메인' 'ⓒ', '배포', '한겨례', '한겨례신문', '▶', '◀', '네이버', '[', ']', 'co', 'kr', 'hani']\n",
        "  tokens = [token for token in tokens if token not in stopwords and token not in my_stopwords]\n",
        "  \n",
        "  #numbers are already left out, from tokenizer.nouns()\n",
        "  #tokens = [word for word in tokens if not any(char.isdigit() for char in word)]\n",
        "\n",
        "  preprocessed = ' '.join(tokens)\n",
        "  return preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss7Fv0uIWaon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Customize FLAGS, because I don't find the tf flags easy to use :-(\n",
        "class Flags():\n",
        "  def __init__(self):\n",
        "    self.training_sample_percentage = float(0.7)\n",
        "    self.max_doc_length = int(350)\n",
        "\n",
        "    self.embedding_dim = int(64)\n",
        "    self.filter_sizes = str('3,4,5')\n",
        "    self.num_filters = int(128)\n",
        "    self.dropout_keep_prob = float(0.5)\n",
        "    self.l2_reg_lambda = float(0.0)\n",
        "    self.learning_rate = float(1e-3)\n",
        "\n",
        "    self.batch_size = int(64)\n",
        "    self.num_epochs = int(50)\n",
        "    \n",
        "FLAGS = Flags()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djYsbYNZWcKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive implementation of reshaping y : 0,1,2 => [1,0,0], [0,1,0], [0,0,1]\n",
        "def expand_y(yVec):\n",
        "  nrow = len(yVec)\n",
        "  y_unique = np.unique(yVec)\n",
        "  ncol = len(y_unique)  \n",
        "  out = np.zeros((nrow, ncol))\n",
        "  \n",
        "  for i, y in enumerate(yVec):\n",
        "    for j, val in enumerate(y_unique):\n",
        "      if y==val:\n",
        "        out[i,j] = 1\n",
        "        \n",
        "  return np.array(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l2XBEytWd0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text CNN class. Embedding is done outside a Text CNN object\n",
        "class TextCNN(object):\n",
        "  def __init__(self, sequence_length, num_classes, embedding_size, filter_sizes, num_filters, learning_rate, l2_reg_lambda):\n",
        "    self.input_x = tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name='input_x')\n",
        "    self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
        "    self.dropout_keep_prob = tf.placeholder(tf.float32, name='prob')\n",
        "    \n",
        "    self.embedded_expanded = tf.expand_dims(self.input_x, -1)\n",
        "    \n",
        "    l2_loss = tf.constant(0.0)\n",
        "    \n",
        "    # Architecture\n",
        "    pooled_outputs = []\n",
        "    for i, filter_size in enumerate(filter_sizes):\n",
        "      with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
        "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')\n",
        "        \n",
        "        conv = tf.nn.conv2d(self.embedded_expanded, W, strides=[1,1,1,1], padding='VALID', name='conv')\n",
        "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
        "        pooled = tf.nn.max_pool(h, ksize=[1, sequence_length-filter_size+1, 1, 1], strides=[1,1,1,1], padding='VALID', name='pool')\n",
        "        \n",
        "        pooled_outputs.append(pooled)\n",
        "        \n",
        "    num_filters_total = num_filters*len(filter_sizes)\n",
        "    self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "    self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "    \n",
        "    with tf.name_scope('dropout'):\n",
        "      self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "      \n",
        "    #Scores and predictions\n",
        "    with tf.name_scope('output'):\n",
        "      W = tf.get_variable('W', shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
        "      b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n",
        "      l2_loss += (tf.nn.l2_loss(W) + tf.nn.l2_loss(b))\n",
        "      self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name='scores')\n",
        "      self.predictions = tf.argmax(self.scores, 1, name='predictions')\n",
        "      \n",
        "    #Loss : Cross Entropy\n",
        "    with tf.name_scope('loss'):\n",
        "      losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "      self.loss = tf.reduce_mean(losses) + l2_reg_lambda*l2_loss\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
        "      \n",
        "    #Performance\n",
        "    with tf.name_scope('accuracy'):\n",
        "      correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P386boGxWjU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure\n",
        "def train(x_train, y_train, num_epochs):\n",
        "  \n",
        "  print('Training Text CNN, with parameters \\nnum_epochs:{:d}, \\nlearning_rate:{:f}, \\\n",
        "  \\nembedding_dim:{:d}, \\nmax_doc_length:{:d}, \\nfilter_sizes:{:s}, \\nnum_filters:{:d}, \\nbatch_size:{:d}\\n\\n'.format(\\\n",
        "        FLAGS.num_epochs, FLAGS.learning_rate, FLAGS.embedding_dim, FLAGS.max_doc_length, FLAGS.filter_sizes, FLAGS.num_filters, FLAGS.batch_size))\n",
        "  \n",
        "  with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "      cnn = TextCNN(sequence_length = FLAGS.max_doc_length,\n",
        "                   num_classes = y_train.shape[1],\n",
        "                   embedding_size = FLAGS.embedding_dim,\n",
        "                   filter_sizes = list(map(int, FLAGS.filter_sizes.split(','))),\n",
        "                   num_filters = FLAGS.num_filters,\n",
        "                   learning_rate = FLAGS.learning_rate,\n",
        "                   l2_reg_lambda = FLAGS.l2_reg_lambda)\n",
        "      \n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      \n",
        "      data_size = len(y_train)\n",
        "      num_batches_per_epoch = int((data_size-1)/FLAGS.batch_size) + 1\n",
        "      \n",
        "      for epoch in range(num_epochs):\n",
        "        total_cost = 0\n",
        "        total_accr = 0\n",
        "        \n",
        "        shuffled_indeces = np.random.permutation(np.arange(data_size))\n",
        "        shuffled_x = [x_train[i] for i in shuffled_indeces]\n",
        "        shuffled_y = y_train[shuffled_indeces]\n",
        "        \n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "          start_idx = batch_num * FLAGS.batch_size\n",
        "          end_idx = min((batch_num+1)*FLAGS.batch_size, data_size)\n",
        "          batch_xs = shuffled_x[start_idx:end_idx]\n",
        "          batch_ys = shuffled_y[start_idx:end_idx]\n",
        "          \n",
        "          feed_dict = {cnn.input_x:batch_xs, cnn.input_y:batch_ys, cnn.dropout_keep_prob:FLAGS.dropout_keep_prob}\n",
        "          _, cost_val, accuracy = sess.run([cnn.optimizer, cnn.loss, cnn.accuracy], feed_dict)\n",
        "          total_cost += cost_val\n",
        "          total_accr += accuracy\n",
        "          \n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'Avg. cost =', '{:.3f}'.format(total_cost / num_batches_per_epoch), 'Avg. accuracy =','{:.3f}'.format(total_accr / num_batches_per_epoch))\n",
        "        \n",
        "      print('Training Complete!')\n",
        "      \n",
        "    return sess, cnn\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nArD6g7WlDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing procedure\n",
        "def test(x_test, y_test, sess, cnn):\n",
        "  feed_dict = {cnn.input_x:x_test, cnn.input_y:y_test, cnn.dropout_keep_prob : 1.0}\n",
        "  print('Test set Accuracy : ', sess.run(cnn.accuracy, feed_dict = feed_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngQiW6vW5BX",
        "colab_type": "text"
      },
      "source": [
        "Now let's get started!! <br>\n",
        "Say we want to see if the political articles in 조선일보 can be told apart from the political articles in 한겨례"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6wg-y7JvvSZ",
        "colab_type": "text"
      },
      "source": [
        "First, we import data (500 per class in this example) and do some preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtg2x0aKW8zt",
        "colab_type": "text"
      },
      "source": [
        " - Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwpCkPhbV838",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2aab780-cab6-4440-cd91-9d47d5bf69f6"
      },
      "source": [
        "# prepare corpus  \n",
        "\n",
        "nrow_per_type = 500\n",
        "ntype = 2\n",
        "\n",
        "data, label = np.empty([nrow_per_type * ntype,1], dtype=object), np.zeros((nrow_per_type * ntype, 1))\n",
        "\n",
        "files_0 = glob.glob(base_dir + 'Chosun/politics/*.txt')\n",
        "files_1 = glob.glob(base_dir + 'Han/politics/*.txt')\n",
        "\n",
        "files_total = np.append(files_0[:nrow_per_type], files_1[:nrow_per_type])\n",
        "\n",
        "print('reading data...')\n",
        "for i, name in enumerate(files_total):\n",
        "  with open(name,'r') as handle:\n",
        "    data[i,0] = handle.read()\n",
        "    label[i,0] = int(i/nrow_per_type)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bS_2jGCwojt",
        "colab_type": "text"
      },
      "source": [
        " - Shuffle and split the prepared data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECdnn96WAGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle and split into train/test set\n",
        "\n",
        "train_percentage = FLAGS.training_sample_percentage\n",
        "\n",
        "data_size = len(label)\n",
        "shuffled_indeces = np.random.permutation(np.arange(data_size))\n",
        "shuffled_data = data[shuffled_indeces]\n",
        "shuffled_label = label[shuffled_indeces]\n",
        "\n",
        "split_idx = int(data_size*train_percentage)\n",
        "\n",
        "data_train = shuffled_data[:split_idx]\n",
        "data_test = shuffled_data[split_idx:]\n",
        "label_train = shuffled_label[:split_idx]\n",
        "label_test = shuffled_label[split_idx:]\n",
        "\n",
        "del shuffled_data, shuffled_label, data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v91UHBxw4zw",
        "colab_type": "text"
      },
      "source": [
        " - Preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrX9ZTejWuTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f54dd4ba-ee3c-428f-bcb0-86422d661a63"
      },
      "source": [
        "print('processing training data(x)...')\n",
        "x_in_train = [preprocess_single_doc(x[0], 3) for x in data_train]\n",
        "print('processing training data(y)...')\n",
        "y_train = expand_y(label_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing training data(x)...\n",
            "processing training data(y)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqOywiM14P_",
        "colab_type": "text"
      },
      "source": [
        " - Train word2vec for the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVpA6cnxXk6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6f79861-88d8-4b4f-c7b3-b8e576ce04d1"
      },
      "source": [
        "print('training word2vec..., for training set')\n",
        "#Now want each word in x_in_train to be represented as vectors!\n",
        "train_corpus = [nltk.word_tokenize(sentence) for sentence in x_in_train]\n",
        "wv_model_trainset = Word2Vec(size=FLAGS.embedding_dim, min_count=2)\n",
        "wv_model_trainset.build_vocab(train_corpus)\n",
        "wv_model_trainset.train(train_corpus, total_examples=wv_model_trainset.corpus_count, epochs=40)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training word2vec..., for training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7231318, 7920320)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo3-ACpT2UNY",
        "colab_type": "text"
      },
      "source": [
        " - The length of each input must be fixed. If longer than 'max_doc_length', crop off the rest. If shorter, then add padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTE_U3A8cYd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_doc_length = FLAGS.max_doc_length\n",
        "x_train = []\n",
        "for i, sent in enumerate(x_in_train):\n",
        "  words = sent.split(' ')\n",
        "  word_cnt = 0\n",
        "  sentList = []\n",
        "  for word in words:\n",
        "    if word in wv_model_trainset.wv.vocab:\n",
        "      if word_cnt < max_doc_length:\n",
        "        wordVec = wv_model_trainset[word]\n",
        "        sentList.append(wordVec)\n",
        "        word_cnt += 1\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "  if len(sentList) < max_doc_length:\n",
        "    last_idx = len(sentList)-1\n",
        "    for j in range(last_idx+1, max_doc_length):\n",
        "      sentList.append(np.zeros(FLAGS.embedding_dim))\n",
        "  \n",
        "  x_train.append(sentList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGRkPV-s2pcf",
        "colab_type": "text"
      },
      "source": [
        "Train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9-UR8HAgpoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "806b6e28-34bd-4228-9617-0c27a639e918"
      },
      "source": [
        "session, cnnModel = train(x_train, y_train, FLAGS.num_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0712 08:57:57.745912 139960581597056 deprecation.py:506] From <ipython-input-8-56bcd47713fe>:30: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Text CNN, with parameters \n",
            "num_epochs:50, \n",
            "learning_rate:0.001000,   \n",
            "embedding_dim:64, \n",
            "max_doc_length:350, \n",
            "filter_sizes:3,4,5, \n",
            "num_filters:128, \n",
            "batch_size:64\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0712 08:57:58.490174 139960581597056 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0712 08:57:58.516240 139960581597056 deprecation.py:323] From <ipython-input-8-56bcd47713fe>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 Avg. cost = 2.943 Avg. accuracy = 0.631\n",
            "Epoch: 0002 Avg. cost = 1.295 Avg. accuracy = 0.846\n",
            "Epoch: 0003 Avg. cost = 0.739 Avg. accuracy = 0.900\n",
            "Epoch: 0004 Avg. cost = 0.609 Avg. accuracy = 0.914\n",
            "Epoch: 0005 Avg. cost = 0.549 Avg. accuracy = 0.929\n",
            "Epoch: 0006 Avg. cost = 0.464 Avg. accuracy = 0.929\n",
            "Epoch: 0007 Avg. cost = 0.529 Avg. accuracy = 0.927\n",
            "Epoch: 0008 Avg. cost = 0.346 Avg. accuracy = 0.956\n",
            "Epoch: 0009 Avg. cost = 0.308 Avg. accuracy = 0.952\n",
            "Epoch: 0010 Avg. cost = 0.283 Avg. accuracy = 0.951\n",
            "Epoch: 0011 Avg. cost = 0.329 Avg. accuracy = 0.947\n",
            "Epoch: 0012 Avg. cost = 0.241 Avg. accuracy = 0.964\n",
            "Epoch: 0013 Avg. cost = 0.243 Avg. accuracy = 0.959\n",
            "Epoch: 0014 Avg. cost = 0.191 Avg. accuracy = 0.963\n",
            "Epoch: 0015 Avg. cost = 0.316 Avg. accuracy = 0.952\n",
            "Epoch: 0016 Avg. cost = 0.179 Avg. accuracy = 0.961\n",
            "Epoch: 0017 Avg. cost = 0.138 Avg. accuracy = 0.976\n",
            "Epoch: 0018 Avg. cost = 0.140 Avg. accuracy = 0.971\n",
            "Epoch: 0019 Avg. cost = 0.122 Avg. accuracy = 0.970\n",
            "Epoch: 0020 Avg. cost = 0.116 Avg. accuracy = 0.974\n",
            "Epoch: 0021 Avg. cost = 0.182 Avg. accuracy = 0.974\n",
            "Epoch: 0022 Avg. cost = 0.108 Avg. accuracy = 0.984\n",
            "Epoch: 0023 Avg. cost = 0.156 Avg. accuracy = 0.970\n",
            "Epoch: 0024 Avg. cost = 0.088 Avg. accuracy = 0.978\n",
            "Epoch: 0025 Avg. cost = 0.091 Avg. accuracy = 0.979\n",
            "Epoch: 0026 Avg. cost = 0.106 Avg. accuracy = 0.980\n",
            "Epoch: 0027 Avg. cost = 0.089 Avg. accuracy = 0.974\n",
            "Epoch: 0028 Avg. cost = 0.104 Avg. accuracy = 0.980\n",
            "Epoch: 0029 Avg. cost = 0.097 Avg. accuracy = 0.982\n",
            "Epoch: 0030 Avg. cost = 0.070 Avg. accuracy = 0.986\n",
            "Epoch: 0031 Avg. cost = 0.076 Avg. accuracy = 0.989\n",
            "Epoch: 0032 Avg. cost = 0.073 Avg. accuracy = 0.989\n",
            "Epoch: 0033 Avg. cost = 0.047 Avg. accuracy = 0.989\n",
            "Epoch: 0034 Avg. cost = 0.073 Avg. accuracy = 0.986\n",
            "Epoch: 0035 Avg. cost = 0.026 Avg. accuracy = 0.989\n",
            "Epoch: 0036 Avg. cost = 0.060 Avg. accuracy = 0.986\n",
            "Epoch: 0037 Avg. cost = 0.041 Avg. accuracy = 0.986\n",
            "Epoch: 0038 Avg. cost = 0.085 Avg. accuracy = 0.989\n",
            "Epoch: 0039 Avg. cost = 0.078 Avg. accuracy = 0.986\n",
            "Epoch: 0040 Avg. cost = 0.078 Avg. accuracy = 0.981\n",
            "Epoch: 0041 Avg. cost = 0.076 Avg. accuracy = 0.987\n",
            "Epoch: 0042 Avg. cost = 0.052 Avg. accuracy = 0.990\n",
            "Epoch: 0043 Avg. cost = 0.065 Avg. accuracy = 0.987\n",
            "Epoch: 0044 Avg. cost = 0.054 Avg. accuracy = 0.984\n",
            "Epoch: 0045 Avg. cost = 0.024 Avg. accuracy = 0.990\n",
            "Epoch: 0046 Avg. cost = 0.039 Avg. accuracy = 0.990\n",
            "Epoch: 0047 Avg. cost = 0.046 Avg. accuracy = 0.993\n",
            "Epoch: 0048 Avg. cost = 0.022 Avg. accuracy = 0.993\n",
            "Epoch: 0049 Avg. cost = 0.056 Avg. accuracy = 0.989\n",
            "Epoch: 0050 Avg. cost = 0.049 Avg. accuracy = 0.990\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-3CwW2P9SQU",
        "colab_type": "text"
      },
      "source": [
        "Training is complete, so now, test performance with test data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BGYj1Ek21vL",
        "colab_type": "text"
      },
      "source": [
        " - Preprocess and reshape the test dataset in the same manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WMSK9OX9qA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5659eba1-4e8a-46fc-b373-e3187704be4b"
      },
      "source": [
        "print('processing test data(x)...')\n",
        "x_in_test = [preprocess_single_doc(x[0], 3) for x in data_test]\n",
        "print('processing test data(y)...')\n",
        "y_test = expand_y(label_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing test data(x)...\n",
            "processing test data(y)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGnmirhF9gAK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "364dcf7e-c56f-4ff3-e5a9-29363ce5e928"
      },
      "source": [
        "print('training word2vec..., for test set')\n",
        "#Now want each word in x_in_train to be represented as vectors!\n",
        "test_corpus = [nltk.word_tokenize(sentence) for sentence in x_in_test]\n",
        "wv_model_testset = Word2Vec(size=FLAGS.embedding_dim, min_count=2)\n",
        "wv_model_testset.build_vocab(test_corpus)\n",
        "wv_model_testset.train(test_corpus, total_examples=wv_model_testset.corpus_count, epochs=40)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training word2vec..., for test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3124965, 3487720)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NOr08zF69gAb",
        "colab": {}
      },
      "source": [
        "max_doc_length = FLAGS.max_doc_length\n",
        "x_test = []\n",
        "for i, sent in enumerate(x_in_test):\n",
        "  words = sent.split(' ')\n",
        "  word_cnt = 0\n",
        "  sentList = []\n",
        "  for word in words:\n",
        "    if word in wv_model_testset.wv.vocab:\n",
        "      if word_cnt < max_doc_length:\n",
        "        wordVec = wv_model_testset[word]\n",
        "        sentList.append(wordVec)\n",
        "        word_cnt += 1\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "  if len(sentList) < max_doc_length:\n",
        "    last_idx = len(sentList)-1\n",
        "    for j in range(last_idx+1, max_doc_length):\n",
        "      sentList.append(np.zeros(FLAGS.embedding_dim))\n",
        "  \n",
        "  x_test.append(sentList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfdo0L8hTKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a89149f1-3df8-4b23-8172-d98bf41f9569"
      },
      "source": [
        "test(x_test, y_test, session, cnnModel)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set Accuracy :  0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQQR8CK3Jhb",
        "colab_type": "text"
      },
      "source": [
        "Conclusion : An astounding proportion of 0.95 of the test dataset are correctly classified ('조선일보' / '한겨례')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRt407DVilu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}