{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Analysis_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesahwi/TextAnlaysis/blob/master/Naver_News_Analysis/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSyxV5bvuWky",
        "colab_type": "text"
      },
      "source": [
        "RNN implementation on analyzing Korean news articles <br>\n",
        "(In this particular example, we will tell '조선일보' and '한겨례' political articles apart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vc62EPBuXB0",
        "colab_type": "text"
      },
      "source": [
        "Install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n72_dHmVy4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "badd80fd-750f-4e1e-dc83-6ac42e5111d1"
      },
      "source": [
        "!apt-get install python3-dev; pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6IJPq6uo4S",
        "colab_type": "text"
      },
      "source": [
        "Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJ2FcVPMkBT",
        "colab_type": "code",
        "outputId": "420c083f-d194-4bbb-afb1-d5b8d8bcc535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkwSeburusgA",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zzufRUV5ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import sklearn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import glob\n",
        "import warnings, os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from konlpy.tag import *\n",
        "\n",
        "# 아래와 같은 분석기가 있음.\n",
        "hnm = Hannanum()\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "\n",
        "base_dir = 'gdrive/Shared drives/텍스트마이닝/News Analysis/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-8kPzT4uwhI",
        "colab_type": "text"
      },
      "source": [
        "Import Korean stop words, found in 'https://www.ranks.nl/stopwords/korean'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP6Th-b8WDR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(base_dir+'stopwords.txt', 'r')\n",
        "stopwords = f.read()\n",
        "stopwords = stopwords.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiuTtjKZvTcL",
        "colab_type": "text"
      },
      "source": [
        "Define functions and objects that will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NWEA6TDo6jYh",
        "colab": {}
      },
      "source": [
        "# preprocess, single documents, not whole corpus.\n",
        "# This function uses nouns. For a more sophisticated analysis, using morphs is recommended\n",
        "def preprocess_single_doc(text, tokenizer_type = 1):\n",
        "  \n",
        "  #choose tokenizer\n",
        "  if tokenizer_type == 1:\n",
        "    tokenizer = Hannanum()\n",
        "  elif tokenizer_type == 2:\n",
        "    tokenizer = Kkma()\n",
        "  elif tokenizer_type == 3:\n",
        "    tokenizer = Okt()\n",
        "    \n",
        "  #tokenize\n",
        "  tokens = tokenizer.nouns(text)\n",
        "  \n",
        "  #remove short words, but probably should not apply to Korean\n",
        "  #tokens = [token for token in tokens if len(token) > 1] \n",
        "\n",
        "  #stop words\n",
        "  my_stopwords = ['조선일보', '조선닷컴', '닷컴', 'Chosun', 'Copyrights', '&', '바로가기', '기자', '구독', '메인' 'ⓒ', '배포', '한겨례', '한겨례신문', '▶', '◀', '네이버', '[', ']', 'co', 'kr', 'hani']\n",
        "  tokens = [token for token in tokens if token not in stopwords and token not in my_stopwords]\n",
        "  \n",
        "  #numbers are already left out, from tokenizer.nouns()\n",
        "  #tokens = [word for word in tokens if not any(char.isdigit() for char in word)]\n",
        "\n",
        "  preprocessed = ' '.join(tokens)\n",
        "  return preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IccGaVf06s-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Customize FLAGS, because I don't find the tf flags easy to use :-(\n",
        "class Flags():\n",
        "  def __init__(self):\n",
        "    self.training_sample_percentage = float(0.7)\n",
        "    \n",
        "    self.n_step = int(350)  # how many words there will be per document\n",
        "    self.n_hidden = int(32)  # number of recurrent steps\n",
        "    \n",
        "    self.embedding_dim = int(64)\n",
        "    self.learning_rate = float(1e-3)\n",
        "    self.batch_size = int(64)\n",
        "    self.num_epochs = int(30)\n",
        "    self.cell_type = int(1)\n",
        "    \n",
        "    \n",
        "FLAGS = Flags()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88NHIwSk6kaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive implementation of reshaping y : 0,1,2 => [1,0,0], [0,1,0], [0,0,1]\n",
        "def expand_y(yVec):\n",
        "  nrow = len(yVec)\n",
        "  y_unique = np.unique(yVec)\n",
        "  ncol = len(y_unique)  \n",
        "  out = np.zeros((nrow, ncol))\n",
        "  \n",
        "  for i, y in enumerate(yVec):\n",
        "    for j, val in enumerate(y_unique):\n",
        "      if y==val:\n",
        "        out[i,j] = 1\n",
        "        \n",
        "  return np.array(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_gufayC8TGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN class. Embedding is done outside an RNN object, and fed in\n",
        "class RNN(object):\n",
        "  def __init__(self, learning_rate, embedding_dim, n_step, n_hidden, n_class, cell_type):\n",
        "    self.input_x = tf.placeholder(tf.float32, [None, n_step, embedding_dim])\n",
        "    self.input_y = tf.placeholder(tf.float32, [None, n_class])\n",
        "    \n",
        "    W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
        "    b = tf.Variable(tf.random_normal([n_class]))\n",
        "    \n",
        "    if cell_type==0:\n",
        "      cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "    elif cell_type==1:\n",
        "      cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
        "    elif cell_type==2:\n",
        "      cell = tf.nn.rnn_cell.GRUCell(n_hidden)\n",
        "    \n",
        "    outputs, states = tf.nn.dynamic_rnn(cell, self.input_x, dtype=tf.float32)\n",
        "    #outputs here has shape [batch_size, n_step, n_hidden]. So, reshpae to [batch_size, n_hidden]\n",
        "    outputs = tf.transpose(outputs, [1,0,2])\n",
        "    outputs = outputs[-1]\n",
        "    \n",
        "    self.model = tf.matmul(outputs, W) + b\n",
        "    \n",
        "    \n",
        "    with tf.name_scope('output'):\n",
        "      self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.model, labels=self.input_y))\n",
        "      self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "\n",
        "      is_correct = tf.equal(tf.argmax(self.model, 1), tf.argmax(self.input_y, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zFg5Ys8YiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure\n",
        "def train(x_train, y_train, num_epochs):\n",
        "  \n",
        "  print('Training RNN, with parameters \\nnum_epochs:{:d}, \\nlearning_rate:{:f}, \\\n",
        "  \\nembedding_dim:{:d}, \\nn_step:{:d}, \\nn_hidden:{:d}, \\ncell_type:{:d}, \\nbatch_size:{:d}\\n\\n'.format(\\\n",
        "        FLAGS.num_epochs, FLAGS.learning_rate, FLAGS.embedding_dim, FLAGS.n_step, FLAGS.n_hidden, FLAGS.cell_type, FLAGS.batch_size))\n",
        "  \n",
        "  with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "      rnn = RNN(learning_rate=FLAGS.learning_rate,\n",
        "                embedding_dim=FLAGS.embedding_dim,\n",
        "                n_step=FLAGS.n_step,\n",
        "                n_hidden=FLAGS.n_hidden,\n",
        "                n_class=y_train.shape[1], \n",
        "                cell_type=FLAGS.cell_type)\n",
        "      \n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      \n",
        "      data_size = len(y_train)\n",
        "      num_batches_per_epoch = int((len(y_train) - 1) / FLAGS.batch_size) + 1\n",
        "      \n",
        "      for epoch in range(num_epochs):\n",
        "        total_cost = 0\n",
        "        total_accr = 0\n",
        "        \n",
        "        shuffled_indeces = np.random.permutation(np.arange(len(y_train)))\n",
        "        shuffled_x = [x_train[i] for i in shuffled_indeces]\n",
        "        shuffled_y = y_train[shuffled_indeces]\n",
        "        \n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "          start_idx = batch_num * FLAGS.batch_size\n",
        "          end_idx = min((batch_num + 1) * FLAGS.batch_size, data_size)\n",
        "          batch_xs = shuffled_x[start_idx:end_idx]\n",
        "          batch_ys = shuffled_y[start_idx:end_idx]\n",
        "\n",
        "          feed_dict = {rnn.input_x:batch_xs, rnn.input_y:batch_ys}\n",
        "\n",
        "          _, cost_val, accuracy = sess.run([rnn.optimizer, rnn.cost, rnn.accuracy], feed_dict)\n",
        "          total_cost += cost_val\n",
        "          total_accr += accuracy\n",
        "\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'Avg. cost =', '{:.3f}'.format(total_cost / num_batches_per_epoch), 'Avg. accuracy =','{:.3f}'.format(total_accr / num_batches_per_epoch))\n",
        "\n",
        "      print('Training Complete!')\n",
        "\n",
        "\n",
        "    return sess, rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F94L3-68ae0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing procedure\n",
        "def test(x_test, y_test, sess, rnn):\n",
        "  feed_dict = {rnn.input_x : x_test, rnn.input_y : y_test}\n",
        "  print('Test set Accuracy : ', sess.run(rnn.accuracy, feed_dict=feed_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH6biFo58qdK",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined the classes and functions, let's get it rolling<br>\n",
        "Say we want to see if our model can tell the political articles apart, between '조선일보' and '한겨례'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOnKlMt4HMAo",
        "colab_type": "text"
      },
      "source": [
        "First, we import data (500 per class in this example) and do some preprocessing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM1SEAs_Hhgd",
        "colab_type": "text"
      },
      "source": [
        " - Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwpCkPhbV838",
        "colab_type": "code",
        "outputId": "5dacd26d-f48c-476a-9e7f-4eb94f4cf3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare corpus  \n",
        "\n",
        "nrow_per_type = 500\n",
        "ntype = 2\n",
        "\n",
        "data, label = np.empty([nrow_per_type * ntype,1], dtype=object), np.zeros((nrow_per_type * ntype, 1))\n",
        "\n",
        "files_0 = glob.glob(base_dir + 'Chosun/politics/*.txt')\n",
        "files_1 = glob.glob(base_dir + 'Han/politics/*.txt')\n",
        "\n",
        "files_total = np.append(files_0[:nrow_per_type], files_1[:nrow_per_type])\n",
        "\n",
        "print('reading data...')\n",
        "for i, name in enumerate(files_total):\n",
        "  with open(name,'r') as handle:\n",
        "    data[i,0] = handle.read()\n",
        "    label[i,0] = int(i/nrow_per_type)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bS_2jGCwojt",
        "colab_type": "text"
      },
      "source": [
        " - Shuffle and split the prepared data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECdnn96WAGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle and split into train/test set\n",
        "\n",
        "train_percentage = FLAGS.training_sample_percentage\n",
        "\n",
        "data_size = len(label)\n",
        "shuffled_indeces = np.random.permutation(np.arange(data_size))\n",
        "shuffled_data = data[shuffled_indeces]\n",
        "shuffled_label = label[shuffled_indeces]\n",
        "\n",
        "split_idx = int(data_size*train_percentage)\n",
        "\n",
        "data_train = shuffled_data[:split_idx]\n",
        "data_test = shuffled_data[split_idx:]\n",
        "label_train = shuffled_label[:split_idx]\n",
        "label_test = shuffled_label[split_idx:]\n",
        "\n",
        "del shuffled_data, shuffled_label, data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PEOCGrpHk5O",
        "colab_type": "text"
      },
      "source": [
        " - Preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FpxB1c58qss",
        "colab_type": "code",
        "outputId": "385e41fa-254e-4500-a449-9377b3407b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('processing training data(x)...')\n",
        "x_in_train = [preprocess_single_doc(x[0], 3) for x in data_train]\n",
        "print('processing training data(y)...')\n",
        "y_train = expand_y(label_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing training data(x)...\n",
            "processing training data(y)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7pyZtiyHqY7",
        "colab_type": "text"
      },
      "source": [
        " - Build a word2vec model from our training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Yh-gMr8sf_",
        "colab_type": "code",
        "outputId": "8a62e685-399f-4143-db88-72ce7f38d15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('training word2vec..., for training set')\n",
        "#Now want each word in x_in_train to be represented as vectors!\n",
        "train_corpus = [nltk.word_tokenize(sentence) for sentence in x_in_train]\n",
        "wv_model_trainset = Word2Vec(size=FLAGS.embedding_dim, min_count=2)\n",
        "wv_model_trainset.build_vocab(train_corpus)\n",
        "wv_model_trainset.train(train_corpus, total_examples=wv_model_trainset.corpus_count, epochs=40)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training word2vec..., for training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7321497, 8028760)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq_e68YhHvXo",
        "colab_type": "text"
      },
      "source": [
        " - The length of each input must be fixed. If longer than 'n_step', crop off the rest. If shorter, then add padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmXOhK9E9LmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_step = FLAGS.n_step\n",
        "x_train = []\n",
        "for i, sent in enumerate(x_in_train):\n",
        "  words = sent.split(' ')\n",
        "  word_cnt = 0\n",
        "  sentList = []\n",
        "  for word in words:\n",
        "    if word in wv_model_trainset.wv.vocab:\n",
        "      if word_cnt < n_step:\n",
        "        wordVec = wv_model_trainset[word]\n",
        "        sentList.append(wordVec)\n",
        "        word_cnt += 1\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "  if len(sentList) < n_step:\n",
        "    last_idx = len(sentList)-1\n",
        "    for j in range(last_idx+1, n_step):\n",
        "      sentList.append(np.zeros(FLAGS.embedding_dim))\n",
        "  \n",
        "  x_train.append(sentList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmOC6O2_H6NH",
        "colab_type": "text"
      },
      "source": [
        "Train RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6juVHlu9Opv",
        "colab_type": "code",
        "outputId": "ae370412-c263-4c7f-f13a-1244e9840086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "session, rnnModel = train(x_train, y_train, FLAGS.num_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0712 08:42:09.367014 140449591834496 deprecation.py:323] From <ipython-input-8-8ae6d72a753e>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0712 08:42:09.369862 140449591834496 deprecation.py:323] From <ipython-input-8-8ae6d72a753e>:16: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0712 08:42:09.471462 140449591834496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0712 08:42:09.485844 140449591834496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training RNN, with parameters \n",
            "num_epochs:30, \n",
            "learning_rate:0.001000,   \n",
            "embedding_dim:64, \n",
            "n_step:350, \n",
            "n_hidden:32, \n",
            "cell_type:1, \n",
            "batch_size:64\n",
            "\n",
            "\n",
            "Epoch: 0001 Avg. cost = 1.083 Avg. accuracy = 0.460\n",
            "Epoch: 0002 Avg. cost = 0.807 Avg. accuracy = 0.503\n",
            "Epoch: 0003 Avg. cost = 0.614 Avg. accuracy = 0.669\n",
            "Epoch: 0004 Avg. cost = 0.562 Avg. accuracy = 0.708\n",
            "Epoch: 0005 Avg. cost = 0.521 Avg. accuracy = 0.729\n",
            "Epoch: 0006 Avg. cost = 0.471 Avg. accuracy = 0.765\n",
            "Epoch: 0007 Avg. cost = 0.283 Avg. accuracy = 0.917\n",
            "Epoch: 0008 Avg. cost = 0.045 Avg. accuracy = 0.990\n",
            "Epoch: 0009 Avg. cost = 0.041 Avg. accuracy = 0.991\n",
            "Epoch: 0010 Avg. cost = 0.025 Avg. accuracy = 0.999\n",
            "Epoch: 0011 Avg. cost = 0.020 Avg. accuracy = 1.000\n",
            "Epoch: 0012 Avg. cost = 0.017 Avg. accuracy = 1.000\n",
            "Epoch: 0013 Avg. cost = 0.014 Avg. accuracy = 1.000\n",
            "Epoch: 0014 Avg. cost = 0.012 Avg. accuracy = 1.000\n",
            "Epoch: 0015 Avg. cost = 0.010 Avg. accuracy = 1.000\n",
            "Epoch: 0016 Avg. cost = 0.009 Avg. accuracy = 1.000\n",
            "Epoch: 0017 Avg. cost = 0.008 Avg. accuracy = 1.000\n",
            "Epoch: 0018 Avg. cost = 0.007 Avg. accuracy = 1.000\n",
            "Epoch: 0019 Avg. cost = 0.006 Avg. accuracy = 1.000\n",
            "Epoch: 0020 Avg. cost = 0.005 Avg. accuracy = 1.000\n",
            "Epoch: 0021 Avg. cost = 0.005 Avg. accuracy = 1.000\n",
            "Epoch: 0022 Avg. cost = 0.004 Avg. accuracy = 1.000\n",
            "Epoch: 0023 Avg. cost = 0.004 Avg. accuracy = 1.000\n",
            "Epoch: 0024 Avg. cost = 0.004 Avg. accuracy = 1.000\n",
            "Epoch: 0025 Avg. cost = 0.003 Avg. accuracy = 1.000\n",
            "Epoch: 0026 Avg. cost = 0.015 Avg. accuracy = 0.999\n",
            "Epoch: 0027 Avg. cost = 0.014 Avg. accuracy = 0.999\n",
            "Epoch: 0028 Avg. cost = 0.013 Avg. accuracy = 0.999\n",
            "Epoch: 0029 Avg. cost = 0.013 Avg. accuracy = 0.999\n",
            "Epoch: 0030 Avg. cost = 0.012 Avg. accuracy = 0.999\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOB6h7HjICBy",
        "colab_type": "text"
      },
      "source": [
        "Training is complete, so we test the performance with our test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvwr1LMIJmj",
        "colab_type": "text"
      },
      "source": [
        " - Preprocess and reshpae test data as we did with the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WMSK9OX9qA4",
        "colab_type": "code",
        "outputId": "8a8e3af0-9ac2-4de0-f127-3a8a77b64fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('processing test data(x)...')\n",
        "x_in_test = [preprocess_single_doc(x[0], 3) for x in data_test]\n",
        "print('processing test data(y)...')\n",
        "y_test = expand_y(label_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing test data(x)...\n",
            "processing test data(y)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGnmirhF9gAK",
        "outputId": "739ef39c-edf9-4ddb-d19c-285a3d23a54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('training word2vec..., for test set')\n",
        "#Now want each word in x_in_train to be represented as vectors!\n",
        "test_corpus = [nltk.word_tokenize(sentence) for sentence in x_in_test]\n",
        "wv_model_testset = Word2Vec(size=FLAGS.embedding_dim, min_count=2)\n",
        "wv_model_testset.build_vocab(test_corpus)\n",
        "wv_model_testset.train(test_corpus, total_examples=wv_model_testset.corpus_count, epochs=40)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training word2vec..., for test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3031714, 3379280)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NOr08zF69gAb",
        "colab": {}
      },
      "source": [
        "n_step = FLAGS.n_step\n",
        "x_test = []\n",
        "for i, sent in enumerate(x_in_test):\n",
        "  words = sent.split(' ')\n",
        "  word_cnt = 0\n",
        "  sentList = []\n",
        "  for word in words:\n",
        "    if word in wv_model_testset.wv.vocab:\n",
        "      if word_cnt < n_step:\n",
        "        wordVec = wv_model_testset[word]\n",
        "        sentList.append(wordVec)\n",
        "        word_cnt += 1\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "  if len(sentList) < n_step:\n",
        "    last_idx = len(sentList)-1\n",
        "    for j in range(last_idx+1, n_step):\n",
        "      sentList.append(np.zeros(FLAGS.embedding_dim))\n",
        "  \n",
        "  x_test.append(sentList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFd99yUUIRqm",
        "colab_type": "text"
      },
      "source": [
        "Almost all done! Test the model now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfdo0L8hTKS",
        "colab_type": "code",
        "outputId": "7585e22b-b338-487a-f108-309ad8d6e0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test(x_test, y_test, session, rnnModel)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set Accuracy :  0.77666664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXis8hxKSVV6",
        "colab_type": "text"
      },
      "source": [
        "Using RNN with the above parameters (specified in the flag), 0.7767 of the test set were correctly classified "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9jFeMHXO30R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}