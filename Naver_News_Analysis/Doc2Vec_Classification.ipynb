{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Analysis_Doc2Vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesahwi/TextAnlaysis/blob/master/Naver_News_Analysis/Doc2Vec_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD1WffI0laBN",
        "colab_type": "text"
      },
      "source": [
        "Install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JDJMxKGfU2B",
        "colab_type": "code",
        "outputId": "33701214-c39d-4846-c0ac-57c2f7f1a358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!apt-get install python3-dev; pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: JPype1>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTAB-T1dBhXZ",
        "colab_type": "text"
      },
      "source": [
        "Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VQ6UtAMBfA9",
        "colab_type": "code",
        "outputId": "094df38a-f897-4ba6-a21f-f8e6ab619837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYAVLFAPgfWX",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0-MENhlfITB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import gensim\n",
        "import sklearn\n",
        "import nltk\n",
        "import collections\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings, os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from konlpy.tag import *\n",
        "\n",
        "# 아래와 같은 분석기가 있음.\n",
        "hnm = Hannanum()\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "\n",
        "base_dir = 'gdrive/Shared drives/텍스트마이닝/News Analysis/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP9a9LiNgh35",
        "colab_type": "text"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7d1PmEgkp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ac6c72e-a693-48a1-cf72-3fd1eaa0a3fe"
      },
      "source": [
        "# prepare corpus  \n",
        "\n",
        "nrow_per_type = 500\n",
        "ntype = 2\n",
        "\n",
        "data, label = np.empty([nrow_per_type * ntype,1], dtype=object), np.zeros((nrow_per_type * ntype, 1))\n",
        "\n",
        "files_0 = glob.glob(base_dir + 'Chosun/politics/*.txt')\n",
        "files_1 = glob.glob(base_dir + 'Han/politics/*.txt')\n",
        "\n",
        "files_total = np.append(files_0[:nrow_per_type], files_1[:nrow_per_type])\n",
        "\n",
        "print('reading data...')\n",
        "for i, name in enumerate(files_total):\n",
        "  with open(name,'r') as handle:\n",
        "    data[i,0] = handle.read()\n",
        "    label[i,0] = int(i/nrow_per_type)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkHwdiF3ha__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train/test set\n",
        "\n",
        "train_percentage = 0.7\n",
        "\n",
        "data_size = len(label)\n",
        "shuffled_indeces = np.random.permutation(np.arange(data_size))\n",
        "shuffled_data = data[shuffled_indeces]\n",
        "shuffled_label = label[shuffled_indeces]\n",
        "\n",
        "split_idx = int(data_size*train_percentage)\n",
        "\n",
        "data_train = shuffled_data[:split_idx]\n",
        "data_test = shuffled_data[split_idx:]\n",
        "label_train = shuffled_label[:split_idx]\n",
        "label_test = shuffled_label[split_idx:]\n",
        "\n",
        "del shuffled_data, shuffled_label, data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIrvdVoOkBhA",
        "colab_type": "text"
      },
      "source": [
        "Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXILgD_rfy12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stopwords\n",
        "f = open(base_dir+'stopwords.txt', 'r')\n",
        "stopwords = f.read()\n",
        "stopwords = stopwords.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9kNDmlvfoqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess, single documents, not whole corpus.\n",
        "# This function uses nouns. For a more sophisticated analysis, using morphs is recommended\n",
        "def preprocess_single_doc(text, tokenizer_type = 1):\n",
        "  \n",
        "  #choose tokenizer\n",
        "  if tokenizer_type == 1:\n",
        "    tokenizer = Hannanum()\n",
        "  elif tokenizer_type == 2:\n",
        "    tokenizer = Kkma()\n",
        "  elif tokenizer_type == 3:\n",
        "    tokenizer = Okt()\n",
        "    \n",
        "  #tokenize\n",
        "  tokens = tokenizer.nouns(text)\n",
        "  \n",
        "  #remove short words, but probably should not apply to Korean\n",
        "  #tokens = [token for token in tokens if len(token) > 1] \n",
        "\n",
        "  #stop words\n",
        "  my_stopwords = ['조선일보', '조선닷컴', '닷컴', 'Chosun', 'Copyrights', '&', '바로가기', '기자', '구독', '메인' 'ⓒ', '배포', '한겨례', '한겨례신문', '▶', '◀', '네이버', '[', ']', 'co', 'kr', 'hani']\n",
        "  tokens = [token for token in tokens if token not in stopwords and token not in my_stopwords]\n",
        "  \n",
        "  #numbers are already left out, from tokenizer.nouns()\n",
        "  #tokens = [word for word in tokens if not any(char.isdigit() for char in word)]\n",
        "\n",
        "  preprocessed = ' '.join(tokens)\n",
        "  return preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JucDaDh9S1n",
        "colab": {}
      },
      "source": [
        "train_corpus = [TaggedDocument(words = preprocess_single_doc(d[0], 3), tags=[str(i)]) for i, d in enumerate(data_train)] #'TaggedDocument', to be used for doc2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nArfpczmVqz",
        "colab_type": "text"
      },
      "source": [
        "Train Doc2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbMRZ5KmDOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "model.build_vocab(train_corpus)\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU7hLzS-mFlP",
        "colab_type": "code",
        "outputId": "a8d7035a-3043-436b-8fa6-0244a7c60927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#see how much documents are the most similar to themselves, according to our doc2vec model\n",
        "#(Testing the performance of our doc2vec model)\n",
        "\n",
        "correct_list = []\n",
        "wrong_list = []\n",
        "wrong_id_list = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "  v = model.infer_vector(train_corpus[doc_id].words)\n",
        "  sims = model.docvecs.most_similar([v])\n",
        "  dif = int(sims[0][0]) - doc_id\n",
        "  if dif==0:\n",
        "    correct_list.append(sims)\n",
        "  else:\n",
        "    wrong_list.append(sims)\n",
        "    wrong_id_list.append(doc_id)\n",
        "\n",
        "print(len(correct_list) / len(train_corpus), ' correct')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0  correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-aAHcPgnpq7",
        "colab_type": "text"
      },
      "source": [
        "Evaluate, with logistic regresson and svm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUr1bkwNmGM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDocVec(model):\n",
        "  totLen = len(model.docvecs)\n",
        "  X = [model.docvecs[i] for i in range(totLen)]\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjscvRgRnkcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#logistic regression, svm model building\n",
        "X_train = getDocVec(model)\n",
        "Y_train = label_train[:,0]\n",
        "\n",
        "lm = LogisticRegression()\n",
        "lmfit = lm.fit(X_train, Y_train)\n",
        "y_train_lm = lmfit.predict(X_train)\n",
        "\n",
        "svm = SVC()\n",
        "svmfit = svm.fit(X_train, Y_train)\n",
        "y_train_svm = svmfit.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJXnc461p8pa",
        "colab_type": "code",
        "outputId": "3c2999e8-a023-4724-a93c-5becf8c82fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "col_lm = collections.Counter(y_train_lm - Y_train)\n",
        "col_svm = collections.Counter(y_train_svm - Y_train)\n",
        "print('Logistic Regression Performance with Training set : ', col_lm[0]/len(y_train_lm))\n",
        "print('Support Vector Machine Performance with Training set : ', col_svm[0]/len(y_train_svm))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance with Training set :  0.7228571428571429\n",
            "Support Vector Machine Performance with Training set :  0.9814285714285714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rlc4hvtpJjS",
        "colab_type": "text"
      },
      "source": [
        "Now that we have built logistic regerssion and svm models, it's time to test the overall performance, using test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndEGhUK6pTf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_corpus =[TaggedDocument(words = preprocess_single_doc(d[0], 3), tags=[str(i)]) for i, d in enumerate(data_test)]\n",
        "\n",
        "model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "model2.build_vocab(test_corpus)\n",
        "model2.train(test_corpus, total_examples=model2.corpus_count, epochs=model2.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33vccGglo2EY",
        "colab_type": "code",
        "outputId": "e7408317-8cd7-4443-8e31-1f3577507338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#with test set\n",
        "X_test = getDocVec(model2)\n",
        "Y_test = label_test[:,0]\n",
        "\n",
        "X_test = getDocVec(model2)\n",
        "y_test_lm = lmfit.predict(X_test)\n",
        "y_test_svm = svmfit.predict(X_test)\n",
        "\n",
        "col_lm = collections.Counter(y_test_lm - Y_test)\n",
        "col_svm = collections.Counter(y_test_svm - Y_test)\n",
        "print('Logistic Regression Performance with Test set : ', col_lm[0]/len(y_test_lm))\n",
        "print('Support Vector Machine Performance with Test set : ', col_svm[0]/len(y_test_svm))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance with Test set :  0.6\n",
            "Support Vector Machine Performance with Test set :  0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd0l7QUdZyKN",
        "colab_type": "text"
      },
      "source": [
        "Using Logistic Regression and SVM on the document vectors, around 0.6 of the political documents were classified ('조선일보'/'한겨례')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH8cvZmrbD8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}