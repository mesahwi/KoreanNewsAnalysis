{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2Vec_toy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesahwi/TextAnlaysis/blob/master/Learning/Doc2Vec_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhaHD7jbjYDi",
        "colab_type": "text"
      },
      "source": [
        "Text Classification\n",
        "\n",
        "(Doc2Vec -> Classification) Toy Example, <br>\n",
        "\n",
        "Using 20newsgroups data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw0FIqHKjfq-",
        "colab_type": "text"
      },
      "source": [
        "Step 0 : import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw8OvcTrOrsp",
        "colab_type": "code",
        "outputId": "e6c092de-8d7a-4a3f-9c93-ced0066b4f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "import sklearn\n",
        "import nltk\n",
        "import collections\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['rec.sport.baseball', 'sci.electronics']\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLw5n1tarhRW",
        "colab_type": "text"
      },
      "source": [
        "Step 1 : Gather data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sjGOZ-grgR8",
        "colab_type": "code",
        "outputId": "1307fcfd-5192-44f4-817a-f3d7eaf922e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print('-----Training Dataset Word Count-----')\n",
        "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "length = len(np.unique(data_train.target))\n",
        "for i in range(length):\n",
        "  n = len(np.where(data_train.target==i)[0])\n",
        "  wc = 0\n",
        "  for j in range(n):\n",
        "    wc = wc + len(data_train.data[j])\n",
        "    \n",
        "  print('type',i, ' : ', wc)\n",
        "  i=i+1\n",
        "  \n",
        "print('-----Test Dataset Word Count-----')  \n",
        "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "length = len(np.unique(data_test.target))\n",
        "for i in range(length):\n",
        "  n = len(np.where(data_test.target==i)[0])\n",
        "  wc = 0\n",
        "  for j in range(n):\n",
        "    wc = wc + len(data_test.data[j])\n",
        "    \n",
        "  print('type',i, ' : ', wc)\n",
        "  i=i+1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----Training Dataset Word Count-----\n",
            "type 0  :  789442\n",
            "type 1  :  779070\n",
            "-----Test Dataset Word Count-----\n",
            "type 0  :  585660\n",
            "type 1  :  580836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kOx3rNJr9T8",
        "colab_type": "text"
      },
      "source": [
        "Step 2 : Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tFL6tdKsBKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text, stemmer=False):\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "#   print('tokenized into words')\n",
        "  \n",
        "  tokens = [word.lower() for word in tokens]\n",
        "#   print('lower capitalization')\n",
        "\n",
        "  tokens = [word for word in tokens if len(word) >= 4]\n",
        "#   print('removed short (length<4) words')\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#   print('lemmatized words')\n",
        "  \n",
        "  tokens = [lemmatizer.lemmatize(word, 'v') for word in tokens]\n",
        "#   print('lemmatized verbs')\n",
        "\n",
        "  stop = stopwords.words('english')\n",
        "  my_stopwords = ['from', 'subject', 'line', 'say', 'would', 'like', 'write', 'article', 'organization', 'year', 'university', 'nntp-posting-host', 'reply-to', 'distribution', 'know']\n",
        "  # my_stopwords were chosen in a post-hoc manner\n",
        "  tokens = [token for token in tokens if token not in stop and token not in my_stopwords]\n",
        "#   print('removed stopwords')\n",
        "  \n",
        "  \n",
        "  if stemmer:\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "#     print('stemmed words')\n",
        "  \n",
        "  tokens = [word for word in tokens if not any(char.isdigit() for char in word)]\n",
        "#   print('removed words containing numbers')\n",
        "  \n",
        "#   preprocessed = ' '.join(tokens)\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LnAP93ZsIFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_train = data_train.data\n",
        "train_corpus = [TaggedDocument(words = preprocess(_d), tags=[str(i)]) for i, _d in enumerate(text_train)] #'TaggedDocument', to be used for doc2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8KbU1at3nA",
        "colab_type": "text"
      },
      "source": [
        "Step 3 : Train Doc2vec Model (, and test performance with training data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGQu8ckoso5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "model.build_vocab(train_corpus)\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzbkSvStuX7Y",
        "colab_type": "code",
        "outputId": "b9382426-7ad0-4ff5-9c41-e9ad2caff41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct_list = []\n",
        "wrong_list = []\n",
        "wrong_id_list = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "  v = model.infer_vector(train_corpus[doc_id].words)\n",
        "  sims = model.docvecs.most_similar([v])\n",
        "  dif = int(sims[0][0]) - doc_id\n",
        "  if dif==0:\n",
        "    correct_list.append(sims)\n",
        "  else:\n",
        "    wrong_list.append(sims)\n",
        "    wrong_id_list.append(doc_id)\n",
        "\n",
        "print(len(correct_list) / len(train_corpus), ' correct')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9941077441077442  correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecauSnxOx-w",
        "colab_type": "text"
      },
      "source": [
        "Plugging in the training documents into 'model.docvecs.most_similar()' , we can see that 99.4% of the training documents were most similar to themselves.<br>\n",
        "Therefore, we can see that our doc2vec is working fine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNP2AFaauRHj",
        "colab_type": "text"
      },
      "source": [
        "Step 4 : Train Logistic Regression using Doc2vec vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XPocS8oDEsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDocVec(model):\n",
        "  totLen = len(model.docvecs)\n",
        "  X = [model.docvecs[i] for i in range(totLen)]\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HCPj23vIXub",
        "colab_type": "text"
      },
      "source": [
        "Sidenote : Haven't yet figured out if there is a more efficient way (getting the vectors directly from model.docvecs, not by iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lf49TgeAfye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = getDocVec(model)\n",
        "Y_train = data_train.target\n",
        "\n",
        "lm = LogisticRegression()\n",
        "lmfit = lm.fit(X_train, Y_train)\n",
        "y_train_lm = lmfit.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwHxwgowJ0JS",
        "colab_type": "code",
        "outputId": "8b9caf94-2898-45b3-a0be-d74109eb654d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "col = collections.Counter(y_train_lm - Y_train)\n",
        "print('Logistic Regression Performance with Training set : ', col[0]/len(y_train_lm))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance with Training set :  0.9882154882154882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdQoFr8JK4dS",
        "colab_type": "text"
      },
      "source": [
        "Now that we have assurance the logistic regression model can perform, we bring in the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBikloawQjHB",
        "colab_type": "text"
      },
      "source": [
        "Step 5 : Test Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCM9G2iBK1ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_test = data_test.data\n",
        "test_corpus =[TaggedDocument(words = preprocess(_d), tags=[str(i)]) for i, _d in enumerate(text_test)]\n",
        "\n",
        "model2 = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "model2.build_vocab(test_corpus)\n",
        "model2.train(test_corpus, total_examples=model2.corpus_count, epochs=model2.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xnvwF5rM0dG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b93ce65d-7a56-44e9-c055-c96d4c91cdea"
      },
      "source": [
        "X_test = getDocVec(model2)\n",
        "y_test_lm = lmfit.predict(X_test)\n",
        "col = collections.Counter(y_test_lm - data_test.target)\n",
        "print('Logistic Regression Performance with Test set : ', col[0]/len(y_test_lm))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Performance with Test set :  0.8240506329113924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_8QL8LQvaJ",
        "colab_type": "text"
      },
      "source": [
        "Though far from 98.8% of our training set, 82.4% is still an acceptable performance :-)"
      ]
    }
  ]
}