{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSA_textClassification_toy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesahwi/TextAnlaysis/blob/master/Learning/LSA_textClassification_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9vKFBUHpWsX",
        "colab_type": "text"
      },
      "source": [
        "Text classification using Latent Semantic Analysis,  for testing purposes <br>\n",
        "Using 20newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lEnosJNs0kh",
        "colab_type": "text"
      },
      "source": [
        "Step 0 : Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8a_TKlwsyst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = [\n",
        "    'talk.politics.mideast',\n",
        "    'rec.sport.baseball',\n",
        "    'sci.electronics'\n",
        "]\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYi_guEgphlY",
        "colab_type": "text"
      },
      "source": [
        "Step 1 : Gather Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KejXyQXnpTDH",
        "colab_type": "code",
        "outputId": "6e37c427-f302-41eb-ff1c-ce01641ec779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "print('-----Training Dataset Word Count-----')\n",
        "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "length = len(np.unique(data_train.target))\n",
        "for i in range(length):\n",
        "  n = len(np.where(data_train.target==i)[0])\n",
        "  wc = 0\n",
        "  for j in range(n):\n",
        "    wc = wc + len(data_train.data[j])\n",
        "    \n",
        "  print('type',i, ' : ', wc)\n",
        "  i=i+1\n",
        "  \n",
        "print('-----Test Dataset Word Count-----')  \n",
        "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "length = len(np.unique(data_test.target))\n",
        "for i in range(length):\n",
        "  n = len(np.where(data_test.target==i)[0])\n",
        "  wc = 0\n",
        "  for j in range(n):\n",
        "    wc = wc + len(data_test.data[j])\n",
        "    \n",
        "  print('type',i, ' : ', wc)\n",
        "  i=i+1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----Training Dataset Word Count-----\n",
            "type 0  :  1175617\n",
            "type 1  :  1163580\n",
            "type 2  :  1130432\n",
            "-----Test Dataset Word Count-----\n",
            "type 0  :  735298\n",
            "type 1  :  726829\n",
            "type 2  :  693695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbLh4yulp2UU",
        "colab_type": "text"
      },
      "source": [
        "Step 2 : Preprocess\n",
        " 1. Create a tf-idf (or word-document) matrix\n",
        " 2. Run a SVD on the matrix (A = u Sig v', where \\\n",
        "      u = Word matrix for Topic \\\\\n",
        "      Sig = Topic Strength \\\\\n",
        "      v = Document matrix for Topic)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiVdFHZGuIa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf-idf matrix\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data_train.data)  #X is in CSR format\n",
        "# print(vectorizer.get_feature_names())\n",
        "\n",
        "#SVD - naive : ARPACK\n",
        "# XArr = X.toarray()\n",
        "# U, sig, Vt = np.linalg.svd(XArr)\n",
        "# sigDiag = np.diag(sig)\n",
        "\n",
        "\n",
        "#SVD - faster : randomized solver\n",
        "numComp = len(np.unique(data_train.target))\n",
        "svd = TruncatedSVD(n_components = numComp, n_iter=100)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "X_train = lsa.fit_transform(X) # ~~ np.dot(sigDiag[:numComp, :numComp], Vt[:numComp, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDIxGhiSFJ-p",
        "colab_type": "text"
      },
      "source": [
        "It's worth noting that if you try to run a naive SVD with a sizeable dataset, the memory explodes, so making use of Truncated SVD is recommended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkGrVTvPZdyR",
        "colab_type": "text"
      },
      "source": [
        "Step 3 : Train! (and test performance on training data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n4QcavC_yAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing performance of model on training data\n",
        "gnb = GaussianNB()\n",
        "gnbfit = gnb.fit(X_train, data_train.target)\n",
        "y_retro_NB = gnbfit.predict(X_train)\n",
        "\n",
        "svm = SVC()\n",
        "svmfit = svm.fit(X_train, data_train.target)\n",
        "y_retro_SVM = svmfit.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II8e4Cai0fNc",
        "colab_type": "code",
        "outputId": "4b9c2a39-7be8-4ddc-afbd-b0afe2eb504a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dif = y_retro_NB - data_train.target\n",
        "val, cnt = np.unique(dif, return_counts = True)\n",
        "correctN = dict(zip(val, cnt))[0]\n",
        "print ('Training set Performance NB : ', correctN / sum(cnt))\n",
        "\n",
        "dif = y_retro_SVM - data_train.target\n",
        "val, cnt = np.unique(dif, return_counts = True)\n",
        "correctN = dict(zip(val, cnt))[0]\n",
        "print ('Training set Performance SVM : ', correctN / sum(cnt))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set Performance NB :  0.8664383561643836\n",
            "Training set Performance SVM :  0.8635844748858448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjtD-4ziYCsA",
        "colab_type": "text"
      },
      "source": [
        "Step 4 : Test performance on testing dataset\n",
        "\n",
        "1.   Preprocess test dataset\n",
        "2.   Test performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiLJLXBDauRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X2 = vectorizer.fit_transform(data_test.data)\n",
        "X_test = lsa.fit_transform(X2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iloCHUgNUMS4",
        "colab_type": "code",
        "outputId": "2fff2729-2d76-4c27-8b22-cfc63c12c24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred_NB = gnbfit.predict(X_test)\n",
        "y_pred_SVM = svmfit.predict(X_test)\n",
        "\n",
        "\n",
        "dif = y_pred_NB - data_test.target\n",
        "val, cnt = np.unique(dif, return_counts = True)\n",
        "correctN = dict(zip(val, cnt))[0]\n",
        "print ('Test set Performance NB : ', correctN / sum(cnt))\n",
        "\n",
        "dif = y_pred_SVM - data_test.target\n",
        "val, cnt = np.unique(dif, return_counts = True)\n",
        "correctN = dict(zip(val, cnt))[0]\n",
        "print ('Test set Performance SVM : ', correctN / sum(cnt))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set Performance NB :  0.8001715265866209\n",
            "Test set Performance SVM :  0.8456260720411664\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}